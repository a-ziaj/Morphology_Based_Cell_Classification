#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score


# define all functions
def remove_nans_from_dict(d):
    """
    Remove NaN values from a dictionary of lists.
    """
    return {key: [x for x in value if pd.notna(x)] for key, value in d.items()}

def dict_shape(d):
    """
    Return the shape of a dictionary of lists.
    """
    num_keys = len(d)
    lengths = {key: len(value) for key, value in d.items()}
    return num_keys, lengths

# create a folder to save the results and load the data

folder_name = "results"

os.makedirs(folder_name, exist_ok=True)
file = 'cell_morphology_data.xlsx'

# Process each sheet from the Excel file generated by the Kinetic Cytometer to compile all the information into a single DataFrame

sheet_names = pd.ExcelFile(file).sheet_names
data_dict = {}

for sheet_name in sheet_names:

    df = pd.read_excel(file, sheet_name=sheet_name)
    cleaned_dict = remove_nans_from_dict(df.to_dict(orient='list'))
    flattened_values = [item for sublist in cleaned_dict.values() for item in sublist]
    data_dict[sheet_name] = flattened_values

df = pd.read_excel(file, sheet_name=0)
cleaned_dict = remove_nans_from_dict(df.to_dict(orient='list'))
shape = dict_shape(cleaned_dict)
treatments_list = [key for key, value in shape[1].items() for _ in range(value)]
data_dict['treatment'] = treatments_list
df = pd.DataFrame.from_dict(data_dict)

# Split the dataset into features (X) and target labels (y), then into training and test sets.

X = df.drop("treatment", axis=1).values
y = df["treatment"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Evaluate the performance of various models on the dataset
models = {"Logistic Regression": LogisticRegression(), "KNN": KNeighborsClassifier(), "Decision Tree Classifier": DecisionTreeClassifier()}
results = []

for model in models.values():
  kf = KFold(n_splits=6, random_state=12, shuffle=True)
  cv_results = cross_val_score(model, X_train, y_train, cv=kf)
  results.append(cv_results)
plt.boxplot(results, labels=models.keys())
plt.title('Model performance comparison')
plt.show()
file_path = os.path.join(folder_name, "model_performance_comparison")
plt.savefig(file_path)

# Set up a pipeline to standardize features and apply the K-Nearest Neighbors classifier

steps = [('scaler', StandardScaler()),
        ('knn', KNeighborsClassifier())]

pipeline = Pipeline(steps)
parameters = {'knn__n_neighbors': np.arange(1, 50)}

cv = GridSearchCV(pipeline, param_grid = parameters)
cv.fit(X_train, y_train)
y_pred = cv.predict(X_test)

print(cv.best_score_)
print(cv.best_params_)

#Generate ROC curve and ROC curve values: false positive rate (fpr), true positive rate (tpr), and thresholds

y_test_binary = np.where(y_test == 'EGF', 1, 0) # Convert the true and predicted labels into binary format for ROC curve analysis
y_pred_binary = np.where(y_pred == 'EGF', 1, 0)


fpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_binary)

plt.figure()
plt.plot([0, 1], [0, 1], 'k--')

plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for cell treatment prediction')
file_path = os.path.join(folder_name, "ROC_curve")

plt.savefig(file_path)
plt.show()

# Generate and save evaluation metrics including the confusion matrix, classification report, and ROC AUC score,
# along with the K-Nearest Neighbors classifier's best cross-validation score and parameters

print(confusion_matrix(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(roc_auc_score(y_test_binary, y_pred_binary))

# File path to save the results
file_path = os.path.join(folder_name, "model_evaluation.txt")

# Open the file and write the results
with open(file_path, "w") as file:
    file.write("Best Cross-Validation Score:\n")
    file.write(str(cv.best_score_) + "\n\n")

    file.write("Best Parameters:\n")
    file.write(str(cv.best_params_) + "\n\n")

    file.write("Confusion Matrix:\n")
    file.write(str(confusion_matrix(y_test, y_pred)) + "\n\n")

    file.write("Classification Report:\n")
    file.write(classification_report(y_test, y_pred) + "\n")

    file.write("ROC AUC Score:\n")
    file.write(str(roc_auc_score(y_test_binary, y_pred_binary)) + "\n")
    

print(f"Results saved successfully at {file_path}")


# In[ ]:




